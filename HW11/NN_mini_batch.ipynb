{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_inputs(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    normalized_X = (X - mean) / std\n",
    "    return normalized_X, mean, std\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        self.bias = np.zeros((1, n_neurons))\n",
    "\n",
    "    def update(self, dW, dB, learning_rate, lambda_reg, n_samples):\n",
    "        # Adjust for L2 regularization\n",
    "        self.weights -= learning_rate * (dW + lambda_reg * self.weights / n_samples)\n",
    "        self.bias -= learning_rate * dB\n",
    "\n",
    "class ActivationFunction:\n",
    "    @staticmethod\n",
    "    def relu(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(dA, Z):\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(dA, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return dA * s * (1 - s)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, activation_type, dropout_rate=0):\n",
    "        self.parameters = Parameters(n_inputs, n_neurons)\n",
    "        self.activation_type = activation_type\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.Z = np.dot(inputs, self.parameters.weights) + self.parameters.bias\n",
    "        if self.activation_type == 'relu':\n",
    "            self.A = ActivationFunction.relu(self.Z)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            self.A = ActivationFunction.sigmoid(self.Z)\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout_rate > 0:\n",
    "            self.mask = np.random.rand(*self.A.shape) > self.dropout_rate\n",
    "            self.A = self.A * self.mask\n",
    "            self.A /= (1 - self.dropout_rate)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA, prev_activation, learning_rate, lambda_reg):\n",
    "        if self.dropout_rate > 0:\n",
    "            dA = dA * self.mask  # Apply dropout mask to gradient\n",
    "            dA /= (1 - self.dropout_rate)\n",
    "\n",
    "        m = dA.shape[1]\n",
    "        if self.activation_type == 'relu':\n",
    "            dZ = ActivationFunction.relu_derivative(dA, self.Z)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            dZ = ActivationFunction.sigmoid_derivative(dA, self.Z)\n",
    "\n",
    "        dW = np.dot(prev_activation.T, dZ) / m + (lambda_reg * self.parameters.weights / m)  # L2 regularization adjustment\n",
    "        dB = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA_prev = np.dot(dZ, self.parameters.weights.T)\n",
    "\n",
    "        self.parameters.update(dW, dB, learning_rate, lambda_reg, m)\n",
    "        return dA_prev\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "    def backward_pass(self, Y, Y_hat, learning_rate, lambda_reg):\n",
    "        dA = compute_mse_loss_derivative(Y, Y_hat)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            if i > 0:\n",
    "                dA = self.layers[i].backward(dA, self.layers[i-1].A, learning_rate, lambda_reg)\n",
    "            else:  # For the first layer, previous activation is the input X itself\n",
    "                dA = self.layers[i].backward(dA, X, learning_rate, lambda_reg)\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat, lambda_reg):\n",
    "        # Regular loss\n",
    "        mse_loss = np.mean((Y - Y_hat) ** 2)\n",
    "        # L2 regularization loss\n",
    "        l2_loss = sum([np.sum(layer.parameters.weights ** 2) for layer in self.layers])\n",
    "        return mse_loss + (lambda_reg / (2 * Y.shape[0])) * l2_loss\n",
    "    \n",
    "    # MINI BATCH Implementation\n",
    "    def train(self, X, Y, learning_rate, epochs, lambda_reg, batch_size=64):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the dataset at the beginning of each epoch\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            Y_shuffled = Y[permutation]\n",
    "\n",
    "            for batch_index in range(n_batches):\n",
    "                start = batch_index * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "\n",
    "                # Step 2: Feed it to Neural Network for a forward pass\n",
    "                Y_hat = self.forward_pass(X_batch)\n",
    "\n",
    "                # Calculate the loss \n",
    "                loss = self.compute_loss(Y_batch, Y_hat, lambda_reg)\n",
    "\n",
    "                # Step 3 & 4: Backward pass to calculate mean gradients and update weights\n",
    "                self.backward_pass(Y_batch, Y_hat, learning_rate, lambda_reg)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                # Compute the loss on the entire dataset for monitoring\n",
    "                Y_hat_full = self.forward_pass(X)\n",
    "                full_loss = self.compute_loss(Y, Y_hat_full, lambda_reg)\n",
    "                print(f'Epoch {epoch}, Loss: {full_loss}')\n",
    "\n",
    "def compute_mse_loss(Y, Y_hat):\n",
    "    return np.mean((Y - Y_hat) ** 2)\n",
    "\n",
    "def compute_mse_loss_derivative(Y, Y_hat):\n",
    "    return -2 * (Y - Y_hat) / Y.shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
