{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPqA8HUr/oBl8W/agdZ1LaD",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunction:\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return z > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        sig = ActivationFunction.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "class Normalization:\n",
    "    @staticmethod\n",
    "    def standardize(X):\n",
    "        mean = np.mean(X, axis=1, keepdims=True)\n",
    "        std = np.std(X, axis=1, keepdims=True)\n",
    "        epsilon = 1e-8  # Small epsilon value to prevent division by zero\n",
    "        X_standardized = (X - mean) / (std + epsilon)\n",
    "        return X_standardized\n",
    "\n",
    "class MiniBatch:\n",
    "    @staticmethod\n",
    "    def create_batches(X, Y, batch_size):\n",
    "        m = X.shape[1]\n",
    "        mini_batches = []\n",
    "\n",
    "        # Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "        # Partition (except the last mini-batch)\n",
    "        num_complete_minibatches = m // batch_size\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*batch_size : (k+1)*batch_size]\n",
    "            mini_batch_Y = shuffled_Y[:, k*batch_size : (k+1)*batch_size]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < batch_size)\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n",
    "            mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def apply(self, A):\n",
    "        self.mask = np.random.rand(*A.shape) > self.rate\n",
    "        return A * self.mask / (1.0 - self.rate)\n",
    "\n",
    "    def backward_apply(self, dA):\n",
    "        return dA * self.mask / (1.0 - self.rate)\n",
    "\n",
    "class Regularization:\n",
    "    @staticmethod\n",
    "    def l2(lambda_, weights):\n",
    "        return lambda_ * np.sum(np.square(weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def l1(lambda_, weights):\n",
    "        return lambda_ * np.sum(np.abs(weights))\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, learning_rate=0.01, regularization=None, lambda_=0.01, dropout_rate=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.aggregate_signal = None\n",
    "        self.activation_output = None\n",
    "\n",
    "    def activate(self, weighted_input, activation):\n",
    "        if activation == \"relu\":\n",
    "            self.activation_output = ActivationFunction.relu(weighted_input)\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation_output = ActivationFunction.sigmoid(weighted_input)\n",
    "        self.aggregate_signal = weighted_input\n",
    "\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def binary_crossentropy(Y, Y_hat):\n",
    "        m = Y.shape[1]\n",
    "        cost = -1/m * np.sum(Y * np.log(Y_hat) + (1-Y) * np.log(1-Y_hat))\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(Y, Y_hat):\n",
    "        m = Y.shape[1]\n",
    "        cost = (1/m) * np.sum((Y - Y_hat)**2)\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    @staticmethod\n",
    "    def rmse(Y, Y_hat):\n",
    "        return np.sqrt(LossFunction.mse(Y, Y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss_derivative(Y, Y_hat, loss_function):\n",
    "        if loss_function == \"binary_crossentropy\":\n",
    "            return - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "        elif loss_function == \"mse\":\n",
    "            return 2 * (Y_hat - Y) / Y.size\n",
    "\n",
    "class GradientDescent:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate):\n",
    "        for layer in layers:\n",
    "            layer.weights -= learning_rate * layer.dW\n",
    "            layer.bias -= learning_rate * layer.db\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_neurons, activation=None):\n",
    "        self.weights = np.random.randn(n_neurons, n_input) * 0.01\n",
    "        self.bias = np.zeros((n_neurons, 1))\n",
    "        self.activation = activation\n",
    "        self.neurons = [Neuron() for _ in range(n_neurons)]\n",
    "        self.activation_output = None \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        Z = np.dot(self.weights, A_prev) + self.bias\n",
    "        A = np.zeros(Z.shape)\n",
    "\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.activate(Z[i], self.activation)\n",
    "            A[i] = neuron.activation_output\n",
    "\n",
    "        self.activation_output = A  # Store the activation output\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA, A_prev):\n",
    "        m = A_prev.shape[1]\n",
    "        dZ = np.zeros(dA.shape)\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            for i, neuron in enumerate(self.neurons):\n",
    "                dZ[i] = dA[i] * ActivationFunction.relu_derivative(neuron.aggregate_signal)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            for i, neuron in enumerate(self.neurons):\n",
    "                dZ[i] = dA[i] * ActivationFunction.sigmoid_derivative(neuron.aggregate_signal)\n",
    "\n",
    "        self.dW = np.dot(dZ, A_prev.T) / m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(self.weights.T, dZ)\n",
    "        return dA_prev\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.dW\n",
    "        self.bias -= learning_rate * self.db\n",
    "\n",
    "class ForwardPropagation:\n",
    "    @staticmethod\n",
    "    def apply(layers, input_data):\n",
    "        A = input_data\n",
    "        for layer in layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "class BackwardPropagation:\n",
    "    @staticmethod\n",
    "    def apply(layers, Y, Y_hat, input_data, loss_function_name):\n",
    "        dA = LossFunction.compute_loss_derivative(Y, Y_hat, loss_function_name)\n",
    "        for i in reversed(range(len(layers))):\n",
    "            layer = layers[i]\n",
    "            prev_A = layers[i-1].activation_output if i > 0 else input_data\n",
    "            dA = layer.backward(dA, prev_A)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_function_name = None\n",
    "        self.loss_function = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss_function):\n",
    "        self.loss_function_name = loss_function\n",
    "        if loss_function in [\"binary_crossentropy\", \"mse\", \"rmse\"]:\n",
    "            self.loss_function = loss_function\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_function}\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        return ForwardPropagation.apply(self.layers, X)\n",
    "\n",
    "    def compute_cost(self, Y, Y_hat):\n",
    "        if self.loss_function == \"binary_crossentropy\":\n",
    "            return LossFunction.binary_crossentropy(Y, Y_hat)\n",
    "        elif self.loss_function == \"mse\":\n",
    "            return LossFunction.mse(Y, Y_hat)\n",
    "        elif self.loss_function == \"rmse\":\n",
    "            return LossFunction.rmse(Y, Y_hat)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    def backward_propagation(self, Y, Y_hat, input_data):\n",
    "        BackwardPropagation.apply(self.layers, Y, Y_hat, input_data, self.loss_function_name)\n",
    "\n",
    "\n",
    "    def update_parameters(self, learning_rate=0.01):\n",
    "        GradientDescent.update_parameters(self.layers, learning_rate)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:43:09.951298Z",
     "start_time": "2024-03-28T19:43:09.938353Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, model, X_train, Y_train, X_val, Y_val, learning_rate=0.01, epochs=50, batch_size=64, normalize=False):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_val = X_val  # Validation data\n",
    "        self.Y_val = Y_val  # Validation labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def execute(self):\n",
    "        if self.normalize:\n",
    "            self.X_train = Normalization.standardize(self.X_train)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            mini_batches = MiniBatch.create_batches(self.X_train, self.Y_train, self.batch_size)\n",
    "            cost_total = 0\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "                Y_hat = self.model.forward_propagation(mini_batch_X)\n",
    "                cost_total += self.model.compute_cost(mini_batch_Y, Y_hat)\n",
    "                self.model.backward_propagation(mini_batch_Y, Y_hat, mini_batch_X)\n",
    "                self.model.update_parameters(self.learning_rate)\n",
    "\n",
    "            cost_avg = cost_total / len(mini_batches)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            Y_train_pred = self.model.forward_propagation(self.X_train)\n",
    "            train_accuracy = self.calculate_accuracy(self.Y_train, Y_train_pred)\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            Y_val_pred = self.model.forward_propagation(self.X_val)\n",
    "            val_accuracy = self.calculate_accuracy(self.Y_val, Y_val_pred)\n",
    "\n",
    "            if epoch % 10 == 0: # Print the cost and accuracy every 10 epochs\n",
    "                print(f\"Epoch {epoch}, Cost: {cost_avg}, Training Accuracy: {train_accuracy*100:.2f}%, Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_accuracy(Y_true, Y_pred):\n",
    "        # Convert predictions to binary outcomes\n",
    "        Y_pred_binary = (Y_pred > 0.5).astype(int)\n",
    "\n",
    "        # Compare predictions with true labels\n",
    "        correct_predictions = np.equal(Y_true, Y_pred_binary)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = np.mean(correct_predictions)\n",
    "\n",
    "        return accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:43:09.955660Z",
     "start_time": "2024-03-28T19:43:09.941412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Layer(n_input=784, n_neurons=10, activation=\"relu\"))  # Input layer with size 784 for MNIST images (28x28)\n",
    "model.add(Layer(n_input=10, n_neurons=8, activation=\"relu\"))    # 2nd layer\n",
    "model.add(Layer(n_input=8, n_neurons=8, activation=\"relu\"))     # 3rd layer\n",
    "model.add(Layer(n_input=8, n_neurons=4, activation=\"relu\"))     # 4th layer\n",
    "model.add(Layer(n_input=4, n_neurons=1, activation=\"sigmoid\"))  # Output layer\n",
    "\n",
    "# Compile the model specifying the loss function\n",
    "model.compile(loss_function=\"binary_crossentropy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:43:09.955780Z",
     "start_time": "2024-03-28T19:43:09.943537Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape((X_train.shape[0], -1)).T / 255.  # Flatten and normalize\n",
    "X_test = X_test.reshape((X_test.shape[0], -1)).T / 255.\n",
    "\n",
    "# Convert labels for binary classification: 1 for digit '5', 0 for all other digits\n",
    "y_train = (y_train == 5).astype(int)\n",
    "y_test = (y_test == 5).astype(int)\n",
    "\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:43:10.346988Z",
     "start_time": "2024-03-28T19:43:09.950069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.625415896877363, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 10, Cost: 0.32257447550797713, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 20, Cost: 0.30597913443944946, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 30, Cost: 0.30387424085877324, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 40, Cost: 0.30352074860202094, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 50, Cost: 0.3033705829021141, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 60, Cost: 0.3033171544382226, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 70, Cost: 0.30335252985278094, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 80, Cost: 0.30335181591935745, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 90, Cost: 0.30331330512737437, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 100, Cost: 0.3033132215874278, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 110, Cost: 0.3033132410979818, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 120, Cost: 0.3033132183323991, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 130, Cost: 0.30331321956295154, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 140, Cost: 0.3034286014250374, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 150, Cost: 0.30331324664486886, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 160, Cost: 0.30350556431721815, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 170, Cost: 0.30335173308949787, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 180, Cost: 0.3035055210471853, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Epoch 190, Cost: 0.3033132386748603, Training Accuracy: 90.96%, Validation Accuracy: 91.08%\n",
      "Test Accuracy: 91.08%\n"
     ]
    }
   ],
   "source": [
    "train = Train(model, X_train, y_train, X_test, y_test, learning_rate=0.001, epochs=200, batch_size=64, normalize=True)\n",
    "train.execute()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_test_pred = model.forward_propagation(X_test) \n",
    "test_accuracy = Train.calculate_accuracy(y_test.reshape(1, -1), Y_test_pred) # Reshape the labels to the correct shape\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:46:55.357647Z",
     "start_time": "2024-03-28T19:43:10.314609Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:46:56.363949Z",
     "start_time": "2024-03-28T19:46:56.362800Z"
    }
   }
  }
 ]
}
